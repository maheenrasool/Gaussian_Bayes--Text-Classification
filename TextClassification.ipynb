{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment No 2c\n",
    "#### Assignment Credit \n",
    "###### *Dr. Sibt ul Hussain*\n",
    "\n",
    "\n",
    "Text Classification (Sentiment Analysis) Using Bayes Rule \n",
    "==============\n",
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "Your goal in this part of assigment is to implement a Naive Bayes Multinomial classifier using  bag of words model for the classification of text (movie reviews) into different categories..\n",
    "\n",
    "**Note** Please note that you are allowed to use only those libraries which we have discussed in the class, i.e. numpy, scipy, pandas.\n",
    "\n",
    "Once you have build and test the model on the provided dataset. You will use the learned techniques to compete in a [Kaggle](https://www.kaggle.com/c/word2vec-nlp-tutorial) competition and report your final score and leaderboard ranking to get full credit.\n",
    "\n",
    "For final submission attach the screen-shot of the leader-board with your score.\n",
    "\n",
    "## Submission Instructions\n",
    "You are required to submit the original notebook file on the Slate (with .ipynb extension), with complete set of outputs. Students failing to do so will get zero marks. \n",
    "\n",
    "*Please read each step carefully and understand it fully before proceeding with code writing*\n",
    "\n",
    "## Plagiarism\n",
    "Any form of plagiarism will not be tolerated and result in 0 marks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import scipy.stats\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "checksum": "41bc7183ffc40d65a148fc72fb955046",
     "grade": false,
     "grade_id": "parse_string",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import bs4\n",
    "\n",
    "def parse_string(string): \n",
    "    \"\"\"\"\n",
    "        Parse the input string and tokenize it using regular expressisons:\n",
    "        First clean the string such that it does not have any punctuation or number, it must only have a-z and A-Z.\n",
    "        Please note that while doing this, the spaces much not get disturbed, but in case of multiple spaces convert \n",
    "        them to one space.\n",
    "        Then convert the string to lower case and return its words as a list of strings.\n",
    "        Example:\n",
    "        --------\n",
    "        Input :  computer scien_tist-s are,,,  the  rock__stars of tomorrow_ <cool>  ????\n",
    "        Output:  ['computer', 'scientists', 'are', 'the', 'rockstars', 'of', 'tomorrow']\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        string: string to be parsed...\n",
    "        re: regular expression to be used for the tokenization.\n",
    "        \n",
    "        Returns:\n",
    "        ---------\n",
    "        list of tokens extracted from the string...\n",
    "    \"\"\"\n",
    "  \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #can we use string library or nltk?\n",
    "    #raise NotImplementedError()\n",
    "    #replace multiple spaces with just one\n",
    "    \n",
    "    string = re.sub(r'<[^>]+>', '', string)\n",
    "    # Removing punctuations in string\n",
    "    # Using regex\n",
    "    string = re.sub(r'[^\\w\\s]|_', '', string)\n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "    #string = re.sub(r'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]','', string)\n",
    "    l = string.lower().split()\n",
    "    #print(l)\n",
    "    return l\n",
    "#parse_string(\"co)*+,-./:;<=>?@[\\\\]mputer  $~  scien_ti%&\\'(^_`{|}st-s are,,,                    the  rock__stars of tomorrow_ <cool>  ????\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a5035e3a9d5a9db47fdbafa62cc8a6a3",
     "grade": false,
     "grade_id": "parse_file",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def parse_file(filename): # Parse a given file\n",
    "    \"\"\"\"\n",
    "        Parameters:\n",
    "        ----------\n",
    "        filename: name of text file to be read\n",
    "   \n",
    "        \n",
    "        Returns:\n",
    "        ---------\n",
    "        read file as raw string (with \\n, \\t, \\r, etc included)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        \n",
    "        content = f.read() #parse_string(f.read()) #if not preprocess here then when?\n",
    "        \n",
    "    \n",
    "    return content\n",
    "\n",
    "#f = parse_file(r\"C:\\Users\\mahee\\Downloads\\Fall24_assignment-2\\Fall24_assignment-2\\data\\imdb1\\pos\\cv997_5046.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "checksum": "6d2b49b37e798724777c6c429add90f1",
     "grade": false,
     "grade_id": "files_to_string",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def files_to_strings(X): #converts file to string\n",
    "    \n",
    "    \"\"\"\n",
    "        Read an array (or list) of files where each file content is read in a string...\n",
    "        Input:\n",
    "        -------\n",
    "        X an array (or list) of file names\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X as a numpy array with each row containing a read string from the file...\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    for file in X:\n",
    "        l.append(parse_file(file))\n",
    "    l = np.array(l, dtype = object) \n",
    "    #print(l.shape)\n",
    "    return l\n",
    "\n",
    "# strs = files_to_strings(np.array([\"./data/imdb1/neg/cv000_29416.txt\", \"./data/imdb1/pos/cv000_29590.txt\"]))\n",
    "# print(strs[1])\n",
    "\n",
    "# stri = parse_string(strs[1])\n",
    "# print(stri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "96714a19700e62e9b60de218a6e81b58",
     "grade": true,
     "grade_id": "test_reading_and_parsing",
     "locked": true,
     "points": 2,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnose\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m assert_equal, assert_list_equal\n\u001b[0;32m      3\u001b[0m assert_list_equal(parse_string(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomputer scien_tist-s are,,,  the  rock__stars of tomorrow_ <cool>  ????\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      4\u001b[0m         [\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomputer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscientists\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mare\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrockstars\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mof\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtomorrow\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect cleanning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m strings \u001b[38;5;241m=\u001b[39m files_to_strings(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/imdb1/neg/cv000_29416.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/imdb1/pos/cv000_29590.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\nose\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnose\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collector, main, run, run_exit, runmodule\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# backwards compatibility\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnose\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SkipTest, DeprecatedTest\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\nose\\core.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01munittest\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnose\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, all_config_files\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnose\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultTestLoader\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnose\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PluginManager, DefaultPluginManager, \\\n\u001b[0;32m     14\u001b[0m      RestrictedPluginManager\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnose\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresult\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextTestResult\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\nose\\loader.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnose\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfailure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Failure\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnose\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnose\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Importer, add_path, remove_path\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnose\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mselector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultSelector, TestAddress\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnose\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m func_lineno, getpackage, isclass, isgenerator, \\\n\u001b[0;32m     24\u001b[0m     ispackage, regex_last_key, resolve_name, transplant_func, \\\n\u001b[0;32m     25\u001b[0m     transplant_class, test_address\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\nose\\importer.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnose\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_module, load_module, acquire_lock, release_lock\n\u001b[0;32m     14\u001b[0m log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imp'"
     ]
    }
   ],
   "source": [
    "from nose.tools import assert_equal, assert_list_equal\n",
    "\n",
    "assert_list_equal(parse_string(\"computer scien_tist-s are,,,  the  rock__stars of tomorrow_ <cool>  ????\"),\n",
    "        [u'computer', u'scientists', u'are', u'the', u'rockstars', u'of', u'tomorrow'], \"Incorrect cleanning\")\n",
    "\n",
    "\n",
    "strings = files_to_strings(np.array([\"./data/imdb1/neg/cv000_29416.txt\", \"./data/imdb1/pos/cv000_29590.txt\"]))\n",
    "with open(\"./data/imdb1/neg/cv000_29416.txt\") as f:\n",
    "    text = f.read()\n",
    "assert_equal(strings[0], text, \"At first index should be text of first file\") # doesn't make sense for this to be here,\n",
    "#strings[0] will be cleaned(one you processed while text has just been read)\n",
    "assert_equal(strings.shape, (2,), \"Shape must be (2,) for two files in list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4412e1df0ae5399bde4cfc7440cdc052",
     "grade": false,
     "grade_id": "classifier",
     "locked": false,
     "solution": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.5, 'pos': 0.5}\n",
      "label:  neg probability:  0.5\n",
      "label:  pos probability:  0.5\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "        \n",
    "## Hint, you can use python dictionary or default dict for counting the words\n",
    "# or counter class from collections \n",
    "\n",
    "#TODO Complete this class for running the complete classifier... \n",
    "\n",
    "#You might need to define auxiliary classes for the complete algorithm..\n",
    "\n",
    "class NaiveBayes:\n",
    "    ''' Implements the Naive Bayes For Text Classification... '''\n",
    "    def __init__(self, classes):\n",
    "        self.classes=classes\n",
    "        self.p_prior = dict.fromkeys(self.classes, 0)\n",
    "        self.uniq = []\n",
    "\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        \n",
    "    def addExample(self, x, y):\n",
    "        '''\n",
    "            Add example to corresponding class model ...\n",
    "            Input\n",
    "            ---------\n",
    "            x: example (list of words)\n",
    "            y: label...\n",
    "        '''\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        #??? what example?\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def train(self, X, Y):\n",
    "        ''' Train the multiclass (or Binary) Bayes Rule using the given \n",
    "            X [m x d] data matrix and Y labels matrix\n",
    "            \n",
    "            Input:\n",
    "            ------\n",
    "            X: [m x d] a data matrix of m d-dimensional examples.\n",
    "            Y: [m x 1] a label vector.\n",
    "            \n",
    "            Returns:\n",
    "            -----------\n",
    "            Nothing\n",
    "            '''\n",
    "        #how will it learn???\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        l = []\n",
    "\n",
    "        for i in X: #for each file in X get cleaned string list\n",
    "            l.append(parse_string(str(i)))\n",
    "        X = np.array(l, dtype = object)\n",
    "        \n",
    "        #create bag of words (tokenize words and count frequency of each word) #bag of words for each doc or all docs?\n",
    "        \n",
    "        j = ''\n",
    "        u = []\n",
    "        for i in X:  # file i in X\n",
    "            for j in i: #word j in file i\n",
    "                u.append(j)\n",
    "        self.uniq = np.array(list(set(u))) #list of all features\n",
    "        bow = dict.fromkeys(self.uniq, 0) # dictionary for bow(one bag for all docs)\n",
    "        for i in X:\n",
    "             #count frequency\n",
    "            for j in i:\n",
    "                bow[j] += 1\n",
    "            \n",
    "        \n",
    "        #already in unigrams as unigrams contain just words\n",
    "\n",
    "        #labels = list(set(Y))\n",
    "        \n",
    "        \n",
    "        for i in Y:       #to calculate prior probability \n",
    "            self.p_prior[i] += 1\n",
    "        for key, val in self.p_prior.items():  \n",
    "                                            # how to calculate word/label? number of times word appears in all docs/total words in all docs?\n",
    "            self.p_prior[key] /= len(Y)\n",
    "        print(self.p_prior)\n",
    "            \n",
    "        \n",
    "        #create uni-gramsas we will be using naive bayes\n",
    "        #probability of class given features(words)\n",
    "        \n",
    "        #probability(C/doc) = P(word/C) * P(word2/doc)....\n",
    "        #use MAP (Maximum A Posteriori) p(c/X) = P(X1/C) * P(X2/C) * ...*P(C)  \n",
    "        #c = argmax(probs)   \n",
    "        \n",
    "    def test(self, X):\n",
    "        \n",
    "        ''' Test the trained classifiers on the given set of examples \n",
    "        \n",
    "                   \n",
    "            Input:\n",
    "            ------\n",
    "            X: [m x d] a data matrix of m d-dimensional test examples.\n",
    "           \n",
    "            Returns:\n",
    "            -----------\n",
    "                pclass: the predicted class for each example, i.e. to which it belongs\n",
    "        '''\n",
    "        \n",
    "        nexamples, nfeatures=X.shape\n",
    "\n",
    "        l = [] \n",
    "\n",
    "        for i in X: #for each file in X get cleaned string list\n",
    "            l.append(parse_string(str(i)))\n",
    "        X = np.array(l, dtype = object) # each file is in a list of words\n",
    "\n",
    "        # each class has it's own dic and this dic contains all features with their counts \n",
    "        p_evid = dict.fromkeys(self.classes, {}) #calculate likelihood for c1 and c2\n",
    "    \n",
    "        j = ''\n",
    "        u = []\n",
    "        probs = []\n",
    "        for i in X:  # file i in X\n",
    "            for j in i: #word j in file i\n",
    "                u.append(j)\n",
    "            unique = np.array(list(set(u))) #list of all features\n",
    "            for l in self.classes: # against every label add words are labels with 0 as value\n",
    "                p_evid[l] = dict.fromkeys(unique, 0)\n",
    "            \n",
    "            j = ''\n",
    "            for file in X: \n",
    "                for word in file:\n",
    "                    for c in self.classes:\n",
    "                        p_evid[c][word] += 1 #now you have counts of features belonging to class\n",
    "            \n",
    "            pcw = dict.fromkeys(self.classes, 1) #probability of class given word #values set 1 so that they don't become 0\n",
    "            \n",
    "            for label, words in p_evid.items(): #label, dict from p_evid\n",
    "                for w,c in words.items():\n",
    "                    \n",
    "                    #all words belonging to that class\n",
    "                    p_evid[label][w] /= sum(p_evid[label][w]) #likelihood probabilities calculated words\n",
    "                                        #total words\n",
    "                    #get index of label\n",
    "            #print(p_evid)        \n",
    "            for label, words in p_evid.items():\n",
    "                for w,c in words.items(): #no likelihood is 0 here\n",
    "                    pcw[label] *= p_evid[label][w] #multiply likelihoods\n",
    "                pcw[label] *= self.p_prior[label] #multiply prior probability   \n",
    "            \n",
    "            for l, p in pcw.items():\n",
    "                print('label: ', l, \"probability: \", p)\n",
    "            max_key = max(pcw, key=pcw.get) #get key with maximum posterior probability\n",
    "            prob.append(max_key)\n",
    "          \n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        return\n",
    "        \n",
    "    def predict(self, x):\n",
    "        '''\n",
    "            Predict the label of given input example...\n",
    "            \n",
    "            Input\n",
    "            ---------\n",
    "            x: example (list of words)\n",
    "            \n",
    "        '''\n",
    "\n",
    "\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        return\n",
    "    \n",
    "X=files_to_strings(tX) # read files and convert each file into set of strings and return an numpy array\n",
    "X = X.reshape((X.shape[0], 1))\n",
    "#Split the data into two halves training and test set...\n",
    "traindata,trainlabels,testdata,testlabels=t.split_data(X,labels)\n",
    "#Find the classes to train\n",
    "classes=np.unique(labels)\n",
    "nb = NaiveBayes(classes)\n",
    "nb.train(traindata,trainlabels)\n",
    "nb.test(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tools as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "tdir= \"./data/imdb1/\" # training dir...\n",
    "#load data, get list of files for each class...\n",
    "posfiles=t.get_files(tdir+'/pos','*',withpath=True)\n",
    "negfiles=t.get_files(tdir+'/neg','*',withpath=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data Dimensions = (2000,)  Training labels dimensions= (2000,)\n",
      "['./data/imdb1//neg\\\\cv001_19502.txt' './data/imdb1//neg\\\\cv002_17424.txt']\n"
     ]
    }
   ],
   "source": [
    "#generate training and testing data...\n",
    "plabels=['pos']*len(posfiles)\n",
    "nlabels=['neg']*len(negfiles)\n",
    "labels=np.concatenate((plabels,nlabels)) # concatenate the +ve and -ve labels\n",
    "tX=np.concatenate((posfiles,negfiles))\n",
    "print (\"Training data Dimensions =\", tX.shape,\" Training labels dimensions=\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['films adapted from comic books have had plenty of success , whether they\\'re about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there\\'s never really been a comic book like from hell before . \\nfor starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid \\'80s with a 12-part series called the watchmen . \\nto say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd . \\nthe book ( or \" graphic novel , \" if you will ) is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes . \\nin other words , don\\'t dismiss this film because of its source . \\nif you can get past the whole comic book thing , you might find another stumbling block in from hell\\'s directors , albert and allen hughes . \\ngetting the hughes brothers to direct this seems almost as ludicrous as casting carrot top in , well , anything , but riddle me this : who better to direct a film that\\'s set in the ghetto and features really violent street crime than the mad geniuses behind menace ii society ? \\nthe ghetto in question is , of course , whitechapel in 1888 london\\'s east end . \\nit\\'s a filthy , sooty place where the whores ( called \" unfortunates \" ) are starting to get a little nervous about this mysterious psychopath who has been carving through their profession with surgical precision . \\nwhen the first stiff turns up , copper peter godley ( robbie coltrane , the world is not enough ) calls in inspector frederick abberline ( johnny depp , blow ) to crack the case . \\nabberline , a widower , has prophetic dreams he unsuccessfully tries to quell with copious amounts of absinthe and opium . \\nupon arriving in whitechapel , he befriends an unfortunate named mary kelly ( heather graham , say it isn\\'t so ) and proceeds to investigate the horribly gruesome crimes that even the police surgeon can\\'t stomach . \\ni don\\'t think anyone needs to be briefed on jack the ripper , so i won\\'t go into the particulars here , other than to say moore and campbell have a unique and interesting theory about both the identity of the killer and the reasons he chooses to slay . \\nin the comic , they don\\'t bother cloaking the identity of the ripper , but screenwriters terry hayes ( vertical limit ) and rafael yglesias ( les mis ? rables ) do a good job of keeping him hidden from viewers until the very end . \\nit\\'s funny to watch the locals blindly point the finger of blame at jews and indians because , after all , an englishman could never be capable of committing such ghastly acts . \\nand from hell\\'s ending had me whistling the stonecutters song from the simpsons for days ( \" who holds back the electric car/who made steve guttenberg a star ? \" ) . \\ndon\\'t worry - it\\'ll all make sense when you see it . \\nnow onto from hell\\'s appearance : it\\'s certainly dark and bleak enough , and it\\'s surprising to see how much more it looks like a tim burton film than planet of the apes did ( at times , it seems like sleepy hollow 2 ) . \\nthe print i saw wasn\\'t completely finished ( both color and music had not been finalized , so no comments about marilyn manson ) , but cinematographer peter deming ( don\\'t say a word ) ably captures the dreariness of victorian-era london and helped make the flashy killing scenes remind me of the crazy flashbacks in twin peaks , even though the violence in the film pales in comparison to that in the black-and-white comic . \\noscar winner martin childs\\' ( shakespeare in love ) production design turns the original prague surroundings into one creepy place . \\neven the acting in from hell is solid , with the dreamy depp turning in a typically strong performance and deftly handling a british accent . \\nians holm ( joe gould\\'s secret ) and richardson ( 102 dalmatians ) log in great supporting roles , but the big surprise here is graham . \\ni cringed the first time she opened her mouth , imagining her attempt at an irish accent , but it actually wasn\\'t half bad . \\nthe film , however , is all good . \\n2 : 00 - r for strong violence/gore , sexuality , language and drug content \\n']\n",
      " ['every now and then a movie comes along from a suspect studio , with every indication that it will be a stinker , and to everybody\\'s surprise ( perhaps even the studio ) the film becomes a critical darling . \\nmtv films\\' _election , a high school comedy starring matthew broderick and reese witherspoon , is a current example . \\ndid anybody know this film existed a week before it opened ? \\nthe plot is deceptively simple . \\ngeorge washington carver high school is having student elections . \\ntracy flick ( reese witherspoon ) is an over-achiever with her hand raised at nearly every question , way , way , high . \\nmr . \" m \" ( matthew broderick ) , sick of the megalomaniac student , encourages paul , a popular-but-slow jock to run . \\nand paul\\'s nihilistic sister jumps in the race as well , for personal reasons . \\nthe dark side of such sleeper success is that , because expectations were so low going in , the fact that this was quality stuff made the reviews even more enthusiastic than they have any right to be . \\nyou can\\'t help going in with the baggage of glowing reviews , which is in contrast to the negative baggage that the reviewers were likely to have . \\n_election , a good film , does not live up to its hype . \\nwhat makes _election_ so disappointing is that it contains significant plot details lifted directly from _rushmore_ , released a few months earlier . \\nthe similarities are staggering : \\ntracy flick ( _election_ ) is the president of an extraordinary number of clubs , and is involved with the school play . \\nmax fischer ( _rushmore_ ) is the president of an extraordinary number of clubs , and is involved with the school play . \\nthe most significant tension of _election_ is the potential relationship between a teacher and his student . \\nthe most significant tension of _rushmore_ is the potential relationship between a teacher and his student . \\ntracy flick is from a single parent home , which has contributed to her drive . \\nmax fischer is from a single parent home , which has contributed to his drive . \\nthe male bumbling adult in _election_ ( matthew broderick ) pursues an extramarital affair , gets caught , and his whole life is ruined . \\nhe even gets a bee sting . \\nthe male bumbling adult in _rushmore_ ( bill murray ) pursues an extramarital affair , gets caught , and his whole life is ruined . \\nhe gets several bee stings . \\nand so on . \\nwhat happened ? \\nhow is it that an individual screenplay ( _rushmore_ ) and a novel ( _election_ ) contain so many significant plot points , and yet both films were probably not even aware of each other , made from two different studios , from a genre ( the high school geeks revenge movie ) that hadn\\'t been fully formed yet ? \\neven so , the strengths of _election_ rely upon its fantastic performances from broderick , witherspoon , and newcomer jessica campbell , as paul\\'s anti-social sister , tammy . \\nbroderick here is playing the mr . rooney role from _ferris bueller_ , and he seems to be having the most fun he\\'s had since then . \\nwitherspoon is a revelation . \\nit\\'s early in the year , it\\'s a comedy , and teenagers have little clout , but for my money , witherspoon deserves an oscar nomination . \\nand once campbell\\'s character gets going , like in her fantastic speech in the gymnasium , then you\\'re won over . \\none thing that\\'s been bothering me since i\\'ve seen it . \\nthere is an extraordinary amount of sexuality in this film . \\ni suppose that , coming from mtv films , i should expect no less . . . \\nbut the film starts off light and airy , like a sitcom . \\nas the screws tighten , and the tensions mount , alexander payne decides to add elements that , frankly , distract from the story . \\nit is bad enough that mr . m doesn\\'t like tracy\\'s determination to win at all costs , but did they have to throw in the student/teacher relationship ? \\neven so , there\\'s no logical reason why mr . m has an affair when he does . \\nthere\\'s a lot to like in _election_ , but the plot similarities to _rushmore_ , and the tonal nosedive it takes as it gets explicitly sex-driven , mark this as a disappointment . \\n']\n",
      " [\"you've got mail works alot better than it deserves to . \\nin order to make the film a success , all they had to do was cast two extremely popular and attractive stars , have them share the screen for about two hours and then collect the profits . \\nno real acting was involved and there is not an original or inventive bone in it's body ( it's basically a complete re-shoot of the shop around the corner , only adding a few modern twists ) . \\nessentially , it goes against and defies all concepts of good contemporary filmmaking . \\nit's overly sentimental and at times terribly mushy , not to mention very manipulative . \\nbut oh , how enjoyable that manipulation is . \\nbut there must be something other than the casting and manipulation that makes the movie work as well as it does , because i absolutely hated the previous ryan/hanks teaming , sleepless in seattle . \\nit couldn't have been the directing , because both films were helmed by the same woman . \\ni haven't quite yet figured out what i liked so much about you've got mail , but then again , is that really important ? \\nif you like something so much , why even question it ? \\nagain , the storyline is as cliched as they come . \\ntom hanks plays joe fox , the insanely likeable owner of a discount book chain and meg ryan plays kathleen kelley , the even more insanely likeable proprietor of a family-run children's book shop called , in a nice homage , the shop around the corner . \\nfox and kelley soon become bitter rivals because the new fox books store is opening up right across the block from the small business . \\nlittle do they know , they are already in love with each other over the internet , only neither party knows the other person's true identity . \\nthe rest of the story isn't important because all it does is serve as a mere backdrop for the two stars to share the screen . \\nsure , there are some mildly interesting subplots , but they all fail in comparison to the utter cuteness of the main relationship . \\nall of this , of course , leads up to the predictable climax . \\nbut as foreseeable as the ending is , it's so damn cute and well-done that i doubt any movie in the entire year contains a scene the evokes as much pure joy as this part does . \\nwhen ryan discovers the true identity of her online love , i was filled with such , for lack of a better word , happiness that for the first time all year , i actually left the theater smiling . \\n\"]\n",
      " ...\n",
      " ['the kids in the hall are an acquired taste . \\nit took at least a season of watching their show on hbo before i became a believer . \\nmaybe after watching a half dozen kids in the hall movies , they would grow into the big screen . \\nmy recommendation is that , unless you are a big fan of the kids , skip the film . \\nas it is , their first--and most likely only--attempt at a full length film lacks the qualities that made their comedy work on tv . \\na big-budget and glossy production can not make up for a lack of spontaneity that permeates their tv show . \\nthe kids go through the motions , but you get the feeling that they arent really having fun doing so . \\nand this makes it more difficult for the audience to enjoy their antics . \\nbrain candy is a bunch of skits tied together by the story of a pharmaceutical company that develops a new drug to cure depression . \\nin typical sketch-comedy tradition , each actor plays several roles . \\ndoctor cooper ( kevin mcdonald ) and his team create the drug . \\nthen , under pressure from don roritor ( mark mckinney ) , founder and president of roritor pharmaceuticals , dr . cooper releases the drug into the marketplace . \\nthe ensuing distribution of the new happy pill throughout the populace drives the rest of the film . \\nat about 90 minutes , brain candy still seems long . \\nthe best thing about sketch comedy--and the kids are no exception--is the ability to quickly deliver the laughs , then go on to another quick skit . \\nbut with the additional set-up necessary in telling a longer , coherent story , the laughs just dont come fast enough . \\nstrangely , the show is even more tame than it was when on cable tv . \\nthe movie makes several attempts at risqueness--mostly by pointing up the gayness of one of scott thompsons characters--but they seem almost forced ; as if they have to live up to a pg rating . \\none of the best bits , though , does make use of thompsons naked buttocks ; we see him charging into battle--going to have sex with some guys taking a shower . \\nin the classic of this genre , monty python pulled off this delicate balancing act between plot advancement and punchline delivery for most of the holy grail . \\nthe kids , unfortunately , are not up to the task . \\nthere are some amusing moments , to be sure , but not enough to make the experience an enjoyable one . \\n']\n",
      " ['there was a time when john carpenter was a great horror director . \\nof course , his best film was 1978\\'s masterpiece , \" halloween , \" but he also made 1980\\'s \" the fog , \" and 1987\\'s underrated , \" prince of darkness . \" \\nheck , he even made a good film in 1995 , with \" in the mouth of madness . \" \\nbut something terribly wrong happened to him in 1992 , with the terrible comedy , \" memoirs of an invisible man . \" \\nsomehow , carpenter has lost his touch , with junk like his failed 1995 remake of , \" village of the damned , \" to his uninspired 1996 sequel , \" escape from l . a . \" those movies , however , look like cinematic works of art compared to his latest film , \" john carpenter\\'s vampires . \" \\nif i was him , i definately wouldn\\'t want to put my own name in the title . \\nit is a sad state of affairs when carpenter can make something as misguided and flatly written and filmed as , \" vampires . \" \\nthe story is simple . \\njack crow ( james woods ) is a vampire hunter who , along with one of his partners , montoya ( daniel baldwin ) , and a prostitute , katrina ( sheryl lee ) , survives an attack from the master vampire , valek ( thomas ian griffith ) . \\nsince katrina was previously bitten by him , crow takes her along because anyone who is bitten by valek becomes telepathically linked to him until they themselves turn into vampires a couple days later , and crow is hoping to find him with the help of her . \\nit seems valek\\'s mission is to steal a black , wooden cross from a roman catholic church that will enable him to become so powerful that sunlight will not destroy him . \\nmy question is : how many time have we seen this same story played out ? \\nwell , the answer is just about as many times as a better version of the story has been made . \\n \" john carpenter\\'s vampires , \" sadly enough , is one of the most unscary horror films i\\'ve ever seen . \\nin fact , there isn\\'t even one suspenseful moment in the whole 105-minute running time . \\nthe non-stop vampire attack sequences are stylelessly filmed , without any interesting camera work , which is usually a trademark of carpenter\\'s . \\nand then we come to the screenplay , which , as far as i can tell , is nearly non-existent . \\nthere is no story development , and there isn\\'t even an attempt to flesh out the characters . \\njames woods can be a good actor , but he has nothing to do here but to say a couple of \" pseudo \" -clever lines of dialogue . \\ndaniel baldwin has some potential , but his character comes off as being very dense . \\nand sheryl lee ( faring much better as laura palmer in \" twin peaks \" ) , like all of the female characters , plays an offensive stereotypical whore . \\nthere is not an ounce of intelligence , or excitement in , \" john carpenter\\'s vamires , \" which is very disheartening coming from an ex-fan of carpenter\\'s . \\nhe has said that he turned down directing , \" halloween : h20 , \" because he couldn\\'t work up any excitement for it . \\nand yet , when asked about a \" vampires \" sequel , he said he would be happy to do it . \\ni think that\\'s a definite sign that carpenter has finally lost any trace of his lasting talent , not to mention a significant number of iq points . \\n']\n",
      " ['two party guys bob their heads to haddaway\\'s dance hit \" what is love ? \" \\nwhile getting themselves into trouble in nightclub after nightclub . \\nit\\'s barely enough to sustain a three-minute _saturday_night_live_ skit , but _snl_ producer lorne michaels , _clueless_ creator amy heckerling , and paramount pictures saw something in the late night television institution\\'s recurring \" roxbury guys \" sketch that would presumably make a good feature . \\nemphasis on the word \" presumably . \" \\n_a_night_at_the_roxbury_ takes an already-thin concept and tediously stretches it far beyond the breaking point--and that of viewers\\' patience levels . \\nthe first five minutes or so of _roxbury_ play very much like one of the original \" roxbury guys \" skits . \\nwith \" what is love ? \" \\nblaring on the soundtrack , the brotherly duo of doug and steve butabi ( chris kattan and will ferrell ) bob their heads , scope out \" hotties \" at clubs , and then bump a select few with violent pelvic thrusts . \\nthere is one crucial difference , however--these guys speak . \\nthat little fact has been used as justification for the film\\'s existence , that the butabis\\' newfound capacity for speech would open up a whole new set of doors for the characters . \\nthe doors opened by director john fortenberry and screenwriters steve koren , ferrell , and kattan are new , that\\'s for sure , but they all lead to comic dead ends . \\nthere is no story per se , only a loosely structured and linked series of subplots . \\nthe brothers literally run into ( or , rather , get run into , as in by car ) richard grieco of _21_jump_street_ fame , and through him they gain entrance into the exclusive roxbury club . \\nthere , they meet a hotshot club owner ( chazz palminteri , conspicuously uncredited--can you blame him ? ) , who takes an interest in an idea of theirs . \\nmeanwhile , the bros\\' overbearing father ( dan hedaya ) wants them to stop clubbing . \\nwhen doug refuses and the dimwitted steve obeys his father , a rift is created between the two . \\nthe narrative messiness of _roxbury_ would have been forgivable if all that went on were the slightest bit funny , but virtually none of it is . \\nthe assembled press audience mostly sat stonily silent throughout the entire film , with the one big exception being a big laugh near the end . \\nalas , the joke--a rather lazy takeoff on _jerry_maguire_--will only strike a chord with people who have seen that film . \\ngranted , a lot of people _have_ seen _jerry_maguire_ , but the fact that the film\\'s best joke is completely dependent on one\\'s familiarity with another film says a lot about _roxbury_\\'s lack of inspiration . \\nthat lack of inspiration can be traced back to the insipid characters themselves . \\nlike too many of the skits on the current incarnation of _saturday_night_live_ , \" the roxbury guys \" is a one-joke sketch that never once suggests that the characters have enough comic life in them to survive outside of the sketch context . \\nafter watching one of the \" roxbury \" skits on snl , this is what you come away with from the characters : they bob their heads to \" what is love ? \" , \\nbump unsuspecting women , and . . . that\\'s all . \\nafter watching _a_night_at_the_roxbury_ , you\\'ll be left with exactly the same . \\n']]\n"
     ]
    }
   ],
   "source": [
    "X=files_to_strings(tX) # read files and convert each file into set of strings and return an numpy array\n",
    "X = X.reshape((X.shape[0], 1))\n",
    "#Split the data into two halves training and test set...\n",
    "traindata,trainlabels,testdata,testlabels=t.split_data(X,labels)\n",
    "#Find the classes to train\n",
    "classes=np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] training a classifier for following classes neg, pos\n",
      "{'neg': 0.5, 'pos': 0.5}\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m nb\u001b[38;5;241m=\u001b[39mNaiveBayes(classes)\n\u001b[0;32m      4\u001b[0m nb\u001b[38;5;241m.\u001b[39mtrain(traindata,trainlabels)\n\u001b[1;32m----> 5\u001b[0m pclasses\u001b[38;5;241m=\u001b[39mnb\u001b[38;5;241m.\u001b[39mtest(testdata)\n\u001b[0;32m      6\u001b[0m acc\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(pclasses\u001b[38;5;241m==\u001b[39mtestlabels)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(testlabels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Info] Accuracy = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(acc))\n",
      "Cell \u001b[1;32mIn[27], line 142\u001b[0m, in \u001b[0;36mNaiveBayes.test\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    139\u001b[0m nexamples, nfeatures\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Now build a Naive Bayes classifier and test it...\n",
    "print ('[Info] training a classifier for following classes {}, {}'.format(classes[0],classes[1]))\n",
    "nb=NaiveBayes(classes)\n",
    "nb.train(traindata,trainlabels)\n",
    "pclasses=nb.test(testdata)\n",
    "acc=np.sum(pclasses==testlabels)/float(testlabels.shape[0])\n",
    "print (\"[Info] Accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cells Start\n",
    "#### Do not Modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5822a32f567e3fb7e7f8515eacddec7d",
     "grade": true,
     "grade_id": "test_classifier_shapes",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_in\n",
    "\n",
    "nb=NaiveBayes(classes)\n",
    "nb.train(traindata,trainlabels)\n",
    "assert_equal (nb.test(testdata).shape[0], testdata.shape[0])\n",
    "assert_in( type(nb.predict([\"ok\"])) , [str, np.string_, np.str, np.str_] , \"Predict should return a label \\\n",
    "                                                                                            not list or array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c4695c7d2b42d0e0570c50ab36361afd",
     "grade": true,
     "grade_id": "test_acc",
     "locked": true,
     "points": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_greater\n",
    "\n",
    "nb=NaiveBayes(classes)\n",
    "nb.train(traindata,trainlabels)\n",
    "pclasses=nb.test(testdata)\n",
    "acc=np.sum(pclasses==testlabels)/float(testlabels.shape[0])\n",
    "assert_greater(acc, 0.77, \"Acc must be greater then 77% you are doing something wrong\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "268244e33988983787a2f3e3b8f6eba3",
     "grade": true,
     "grade_id": "test_classifier_responce",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "comment_pos = \"A nice movie, the case was good. Overall a perfect play\"\n",
    "comment_neg = \"A waste of time, cast was bad. a clear No!\"\n",
    "\n",
    "#generate training and testing data...\n",
    "tX=np.concatenate((posfiles,negfiles))\n",
    "X=files_to_strings(tX)\n",
    "X = X.reshape((X.shape[0], 1))\n",
    "\n",
    "plabels=['pos']*len(posfiles)\n",
    "nlabels=['neg']*len(posfiles)\n",
    "true_labels = np.concatenate((plabels,nlabels))\n",
    "inverted_labels = np.concatenate((nlabels,plabels))\n",
    "\n",
    "true_nb=NaiveBayes(classes)\n",
    "true_nb.train(X,true_labels)\n",
    "\n",
    "inverted_nb=NaiveBayes(classes)\n",
    "inverted_nb.train(X,inverted_labels)\n",
    "\n",
    "assert_equal( true_nb.predict(comment_pos.split()), \"pos\" )\n",
    "assert_equal( true_nb.predict(comment_neg.split()), \"neg\" )\n",
    "\n",
    "assert_equal( inverted_nb.predict(comment_pos.split()), \"neg\" )\n",
    "assert_equal( inverted_nb.predict(comment_neg.split()), \"pos\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cells End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "Now lets throw our methods to winds of different folds and measure their accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets generate n-fold training and testing data...\n",
    "nfolds=10\n",
    "folds=t.generate_folds(X,labels,nfolds) # generate folds for \n",
    "for k in arange(len(folds)):\n",
    "    print (folds[k][0].shape, folds[k][2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totacc=[]\n",
    "#train a classifier for each fold...\n",
    "classes=np.unique(labels)\n",
    "\n",
    "for k in range(nfolds):\n",
    "    nb=NaiveBayes(classes)\n",
    "    \n",
    "    traindata=folds[k][0]\n",
    "    trainlabels=folds[k][1]\n",
    "    \n",
    "    #Lets first train the classifier\n",
    "    nb.train(traindata,trainlabels)\n",
    "    \n",
    "    testdata=folds[k][2]\n",
    "    testlabels=folds[k][3]\n",
    "    \n",
    "    #Lets test the classifier\n",
    "    pclasses= nb.test(testdata)\n",
    "    \n",
    "    #print pclasses\n",
    "    acc=np.sum(pclasses==testlabels)/float(testlabels.shape[0])\n",
    "    print (\"[Info] Fold {} Accuracy = {}\".format(k+1, acc))\n",
    "    \n",
    "    totacc.append(acc)\n",
    "\n",
    "print (totacc)\n",
    "\n",
    "mean_acc = np.mean(totacc)\n",
    "print ('[Info] Mean Accuracy =', mean_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Excellent, now its time to go into real waters of Kaggle.\n",
    "\n",
    "\n",
    "You will be needed to create an account on the Kaggle and download the data for the competition [\"Bag of words meets bags of popcorn\"](https://www.kaggle.com/c/word2vec-nlp-tutorial/data).  Note that you will be only downloading the \"labeledTrainData.tsv\" and \"labeledTestData.tsv\".\n",
    "\n",
    "\n",
    "\"labeledTrainData.tsv\" will be used for training your model and thus have prespecified labels for each example review. \"labeledTestData.tsv\" will be used for testing your model and thus don't have prespecified labels for each example. You will predicting the label for each review and then uploading your result to Kaggle server which will be evaluating your model and will give score to your entry. You will report this score during your assignment submission.\n",
    "\n",
    "**[Caution]** Please note that Kaggle limits maximum number of evaluations per 24 hours to 5 to reduce the overfitting on the test set, so be careful and throughly test your model before submitting your entry to Kaggle server. \n",
    "\n",
    "Read the instructions on the Competition Page. Note you are not allowed to use any of the library except what we have learned during class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data-set\n",
    "train=pd.read_csv('/tmp/labeledTrainData.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yt=train['sentiment']\n",
    "Xt=train['review']\n",
    "Xt=np.array(Xt)\n",
    "Yt=np.array(Yt)\n",
    "\n",
    "print (Xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read test set...\n",
    "test=pd.read_csv('/tmp/testData.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "560d69ba70bb62a18e317665c5058abe",
     "grade": false,
     "grade_id": "kaggle_cell_1",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's split the training data into two halves and test our accuracy...\n",
    "traindata,trainlabels,testdata,testlabels=t.split_data(Xt.reshape((Xt.shape[0],1)),Yt)\n",
    "classes=np.unique(trainlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "65200523b8bf7de2418ba0de0fb987c6",
     "grade": false,
     "grade_id": "kaggle_cell_2",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Now lets go and train the model and see its performance...\n",
    "print ('[Info] training a classifier for following classes {}, {}'.format(classes[0],classes[1]))\n",
    "nb=NaiveBayes(classes)\n",
    "nb.train(traindata,trainlabels)\n",
    "pclasses=nb.test(testdata)\n",
    "acc=np.sum(pclasses==testlabels)/float(testlabels.shape[0])\n",
    "print (\"[Info] Accuracy = {}\".format(acc) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation Time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e3f8832c8d5c5aec3a4686f9cf13bf5a",
     "grade": false,
     "grade_id": "kaggle_cell_3",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#Split the training data into 10 folds and test classifiers performance...\n",
    "\n",
    "nfolds=10\n",
    "folds=t.generate_folds(Xt.reshape((Xt.shape[0],1)),Yt,nfolds) # generate folds for \n",
    "for k in arange(len(folds)):\n",
    "    print (folds[k][0].shape, folds[k][2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "85c038543210c9d4b02762be14730d9b",
     "grade": false,
     "grade_id": "kaggle_cell_4",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# As it takes time, so becareful it can cause your machine into red hot oven\n",
    "totacc=[]\n",
    "classes=np.unique(Yt)\n",
    "\n",
    "for k in range(nfolds):\n",
    "    nb=NaiveBayes(classes)\n",
    "    \n",
    "    traindata=folds[k][0]\n",
    "    trainlabels=folds[k][1]\n",
    "    \n",
    "    #Lets first train the classifier\n",
    "    nb.train(traindata,trainlabels)\n",
    "    \n",
    "    testdata=folds[k][2]\n",
    "    testlabels=folds[k][3]\n",
    "    \n",
    "    #Lets test the classifier\n",
    "    pclasses= nb.test(testdata)\n",
    "    \n",
    "    acc=np.sum(pclasses==testlabels)/float(testlabels.shape[0])\n",
    "    print (\"[Info] Fold {} Accuracy = {}\".format(k+1, acc) ) \n",
    "    \n",
    "    totacc.append(acc)\n",
    "\n",
    "print (totacc)\n",
    "print ('[Info] Mean Accuracy =', np.mean(totacc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's train on the complete dataset and test on the provided test set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c27687a3a83576b5360a0e383909305f",
     "grade": false,
     "grade_id": "kaggle_cell_5",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "classes= np.unique(Yt)\n",
    "print ('Training a Classifier on Full training set with classes =', classes)\n",
    "nb=NaiveBayes(classes)\n",
    "nb.train(Xt.reshape(Xt.shape[0],1),Yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "checksum": "2df4444c4b4fcbde8568ffea728d5658",
     "grade": false,
     "grade_id": "kaggle_cell_6",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#Get the test data...\n",
    "Xtest=test['review']\n",
    "Xtest=np.array(Xtest.reshape((Xtest.shape[0],1)))\n",
    "#test the classifier on the provided test set...\n",
    "pclasses=nb.test(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "checksum": "a902480b291b88f6c9fb7422cd845305",
     "grade": false,
     "grade_id": "kaggle_cell_7",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#write the result in the kaggle's required format\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":pclasses} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"/tmp/Naive_bays_Bag_of_Words_model.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to Upload the prediction to Kaggle...\n",
    "\n",
    "Now upload the result on the Kaggle and see your ranking and score. Using this simple method you can have an accuracy of around 0.80960."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement by Excluding Stop Words...\n",
    "\n",
    "You can improve your score further by excluding the commonly occuring words (also known as stop words) in the English language.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read and create a set of stop \n",
    "stopwords=set(t.read_txt_file('./data/sentiment/data/english.stop'))\n",
    "print (stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can re-build the model by excluding these words and again upload your results on Kaggle. \n",
    "\n",
    "Doing this simple trick can further improve your accuracy to 0.81768.\n",
    "\n",
    "For final submission attach the screen-shot of the leader-board with your score\n",
    "\n",
    "Insert ScreenShot of Leader-board Below\n",
    "----------------------------------------\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
